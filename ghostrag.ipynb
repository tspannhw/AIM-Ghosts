{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa05d9d2-c3ae-42b2-8de3-7730e60aa2ec",
   "metadata": {},
   "source": [
    "## Ghost RAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302e9f7-5c19-4ce4-a895-aaffbf1ede7e",
   "metadata": {},
   "source": [
    "# Goal of this Notebook\n",
    "\n",
    "In this notebook we use langchain to build a simple RAG to Ollama and we ask the llama3 model for weather reports from the weather context fed from Milvus.\n",
    "\n",
    "### Simple Retrieval-Augmented Generation (RAG) with LangChain:\n",
    "\n",
    "Build a simple Python [RAG](https://milvus.io/docs/integrate_with_langchain.md) application (streetcamrag.py) to use Milvus for asking about the current weather via OLLAMA.   While outputing to the screen we also send the results to Slack formatted as Markdown.\n",
    "\n",
    "### ðŸ” Summary\n",
    "By the end of this application, youâ€™ll have a comprehensive understanding of using Milvus, data ingest object semi-structured and unstructured data, and using Open Source models to build a robust and efficient data retrieval system.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c891acb2-cd6b-4178-8a60-cd4742d22006",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -Uq langchain-huggingface pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd69da25-234b-4f34-91e6-755ca5f73a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -Uq layoutparser torchvision slack-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b814bd3b-6755-4fca-9c8c-371995930b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "from pymilvus import MilvusClient\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import hub\n",
    "import requests\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Constants\n",
    "MILVUS_URL = \"http://192.168.1.166:19530\" \n",
    "DIMENSION = 512\n",
    "TEXTDIMENSION = 768\n",
    "COLLECTION = \"ghosts\"\n",
    "EMBEDDING_MODEL = \"clip-ViT-B-32\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\", \"trust_remote_code\": True}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL,  model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e1f60b-9cc8-4b1b-b634-25702f76a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "\n",
    "### Turn off slack warnings\n",
    "os.environ[\"SKIP_SLACK_SDK_WARNING\"] = \"false\"\n",
    "\n",
    "slack_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n",
    "slackclient = WebClient(token=slack_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba078b3f-bc0c-4873-914f-65a3c5840c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " In the image, a ghostly figure is standing in the center of a room. The figure is dressed in a long blue sheet, with eye holes cut out, giving it an ethereal appearance. It stands with its arms outstretched, as if reaching for something or someone beyond our view. The background of the image reveals a dimly lit room with a hint of another person in the far distance. The overall atmosphere is one of solitude and mystery. \n",
      "Unstable\n",
      "Class I\n",
      "http://192.168.1.166:9000/images/victorian600.jpg\n"
     ]
    }
   ],
   "source": [
    "# Create the Milvus vector store\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"ghosts\",\n",
    "    primary_field = \"id\",\n",
    "    vector_field=\"vector\",\n",
    "    text_field=\"description\",\n",
    "    connection_args={\"uri\": MILVUS_URL},\n",
    ")\n",
    "\n",
    "results = vector_store.similarity_search(\"Describe any ghosts in the photo\", k=1)\n",
    "\n",
    "print(len(results))\n",
    "print(results[0].page_content) \n",
    "print(results[0].metadata[\"category\"])\n",
    "print(results[0].metadata[\"ghostclass\"])\n",
    "print(results[0].metadata[\"s3path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d2db689-2b04-4ac9-8496-6db0a9d5a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "def run_query() -> None:\n",
    "    llm = OllamaLLM(\n",
    "        model=\"llama3.2\",\n",
    "        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "        stop=[\"<|eot_id|>\"],\n",
    "    )\n",
    "\n",
    "    query = input(\"\\nQuery: \")\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm, retriever=vector_store.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    # print(result)\n",
    "\n",
    "    resultforslack = str(result[\"result\"])\n",
    "    print(resultforslack)\n",
    "\n",
    "    try:\n",
    "        response = slackclient.chat_postMessage(mrkdwn=True, channel=\"C06NE1FU6SE\", text=\"\", \n",
    "                                            blocks=[{\"type\": \"section\",\"text\": {\"type\": \"mrkdwn\",\"text\": \"*\" + str(query) + \"*  \\n\\n\" + str(resultforslack) +\"\\n\" }}])\n",
    "\n",
    "    \n",
    "    except SlackApiError as e:\n",
    "        # You will get a SlackApiError if \"ok\" is False\n",
    "        print(\"Slack failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a272b6-ca39-43d7-935e-8df678a0d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/6wq9klzn19d97gvw8f8j1jdr0000gn/T/ipykernel_62925/562725313.py:3: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  run_query()\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  What do you see?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothyspann/miniforge3/envs/hybridsearch/lib/python3.10/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't see anything explicitly stated in the provided context. The description focuses on the atmosphere and setting of the scene, but doesn't provide any information about what is being seen.I don't see anything explicitly stated in the provided context. The description focuses on the atmosphere and setting of the scene, but doesn't provide any information about what is being seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/6wq9klzn19d97gvw8f8j1jdr0000gn/T/ipykernel_62925/562725313.py:3: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  run_query()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36mrun_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_query\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OllamaLLM(\n\u001b[1;32m      5\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mCallbackManager([StreamingStdOutCallbackHandler()]),\n\u001b[1;32m      7\u001b[0m         stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mQuery: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mpull(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrlm/rag-prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     14\u001b[0m         llm, retriever\u001b[38;5;241m=\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mas_retriever(), chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     15\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/hybridsearch/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hybridsearch/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e09a235-7344-44c3-9c94-cdd6c44546f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "4\n",
      "http://192.168.1.166:9000/images/victorian600.jpg\n"
     ]
    }
   ],
   "source": [
    "# Create the Milvus vector store\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"ghosts\",\n",
    "    primary_field = \"id\",\n",
    "    text_field=\"description\",\n",
    "    vector_field=\"vector\",\n",
    "    connection_args={\"uri\": MILVUS_URL},\n",
    ")\n",
    "\n",
    "results = vector_store.similarity_search(\"ghost\", k=100)\n",
    "print(len(results))\n",
    "#print(results)\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# https://github.com/AlaGrine/RAG_chatabot_with_Langchain/blob/main/RAG_notebook.ipynb\n",
    "# https://colab.research.google.com/drive/1X16irfbWboi7BdyYhnF6QYirRba5doAi?usp=sharing#scrollTo=pyveyZh_LWVJ\n",
    "# https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_multi_modal_RAG_LLaMA2.ipynb?ref=blog.langchain.dev\n",
    "# https://github.com/langchain-ai/langchain/blob/8fea07f92e5c5b80a659b4915f7349babd36fdc6/docs/docs/integrations/retrievers/milvus_hybrid_search.ipynb#L8\n",
    "\n",
    "# build cookbooks like langchain for milvus 101\n",
    "\n",
    "# Use the retriever\n",
    "query = \"Describe the ghost in the photo\"\n",
    "retrieved_docs = retriever.invoke(query) # , limit=10\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "if ( len(retrieved_docs) > 0 ):\n",
    "    #print(retrieved_docs[0])\n",
    "    print(retrieved_docs[0].metadata[\"s3path\"] )\n",
    "    #print(retrieved_docs[1].metadata[\"s3path\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7271e9-a0e7-4e93-8569-5b295c1cc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "import base64\n",
    "from pdf2image import convert_from_path\n",
    "import layoutparser as lp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def plt_img_base64(img):\n",
    "    image_html = f'<img  width=\"200px\" height=\"200px\" src=\"{img}\" />'\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4c7ff5-106e-4a99-9f46-70a99afd2193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img  width=\"200px\" height=\"200px\" src=\"http://192.168.1.166:9000/images/victorian600.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_img_base64(retrieved_docs[0].metadata[\"s3path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10fc1038-f96a-4bc3-8712-21dd6dbb77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_milvus.vectorstores import Milvus\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm_model = OllamaLLM(model=\"llava:7b\",temperature=1, top_p=0.85)\n",
    "\n",
    "def prepare_image_context(docs):\n",
    "    images = []\n",
    "    for doc in docs:\n",
    "        images.append(doc.metadata[\"s3path\"])\n",
    "    return {\"images\": images}\n",
    "\n",
    "def img_prompt(data_dict):\n",
    "    messages = []\n",
    "\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"images\",\n",
    "                \"images\": {\"images\": image},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"Use information contained in the image to provide contextualized answer related to the user question. \\n\"\n",
    "    f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(prepare_image_context),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt)\n",
    "        | llm_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "195ee21d-7c53-4bb5-9111-8be1240140aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, but the image you provided does not depict a ghost or any other supernatural entities. The images show Victorian-style interiors with various decorations and furniture. If you have specific questions about the style of the rooms or the types of objects in the photos, please let me know! \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "chain_multimodal_rag.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b0474-fe40-4d39-b531-ade19ec9dd51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
